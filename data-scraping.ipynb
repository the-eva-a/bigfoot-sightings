{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Bigfoot Data - Collection and Cleaning\n",
    "This jupyter notebook will let us scrape report urls from [The Bigfoot Field Reaserchers Organization (BFRO)](www.bfro.net) and convert them into a structured dataframe. After standard cleaning procedures, the cleaned DataFrame will be saved. \n",
    "## Part I: Data Collection\n",
    "We are collecting Bigfoot sighting data from the BFRO, a group that has been documenting reports since 1995. This extensive collection of entries will provide a robust dataset for exploration. The BFRO considers itself the only *scientific* Bigfoot research group, and therefore holds itself to a high research standard which is reflected in the quality and consistency of reports. This structure will be key to a streamlined scraping process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigfoot_url = 'https://www.bfro.net/'\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='bigfoot_scraper.log',  # Log file name\n",
    "    filemode='a',  # Append mode\n",
    "    level=logging.DEBUG,  # Log everything from DEBUG and above\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soupify_website(url):\n",
    "    \"\"\"\n",
    "    Fetches the HTML content of a given URL and parses it into a BeautifulSoup object.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the webpage to be scraped.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: A BeautifulSoup object representing the parsed HTML of the webpage.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the HTTP response status is not 200(OK)\n",
    "    \"\"\"\n",
    "    \n",
    "    # test connection\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to fetch {url}: Status code {response.status_code}\")\n",
    "\n",
    "def get_links(soup_list):\n",
    "    \"\"\"\n",
    "    Extracts all hyperlinks from a list of BeautifulSoup objects.\n",
    "\n",
    "    Parameters:\n",
    "        soup_list (list): A list of BeautifulSoup objects to search for links.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of hyperlink strings (`href` values) extracted from the provided BeautifulSoup objects.\n",
    "    \n",
    "    Notes:\n",
    "        - Only links with an `href` attribute will be included.\n",
    "        - Duplicate links are not removed; the returned list may contain duplicates.\n",
    "    \"\"\"\n",
    "    link_list = []\n",
    "    for soup in soup_list:\n",
    "        links = soup.find_all('a')\n",
    "        for link in links:\n",
    "            if link.get('href'):\n",
    "                url = link.get('href')\n",
    "                link_list.append(url)\n",
    "    return link_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Report: Our Content\n",
    "To illustrate our scraping process, we will consider [Report #77968](https://www.bfro.net/GDB/show_report.asp?id=77968) from Douglas County Nebraska. While the sections detailing the encounter observations and follow up research are intriguing and could be useful for future expansion, for this ntoebook at least, our interest lies in those first sections of information. \n",
    "<figure>\n",
    "<img src='images/Report%2077968%20relevant%20information.png' width = 500>\n",
    "<figcaption>\n",
    "The relevant information from <a href=\"https://www.bfro.net/GDB/show_report.asp?id=77968\"> Report #77968 </a> that we will be collecting.\n",
    "</figcaption> \n",
    "</figure>\n",
    "\n",
    "These sections provide key details, such as location, date, classification, and reporter. Here we are lucky as each of these sections is seperated from one another by `span` elements, making it a simple task to create a collect the desired inforrmation and store it in a dictionary for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sighting_dictionary(url):\n",
    "    \"\"\"\n",
    "    Extracts information from an individual report page and returns it as a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the report page.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the extracted information, or None if the page cannot be parsed.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Fetching report from: {url}\")\n",
    "    report = soupify_website(url)\n",
    "\n",
    "    if not report:\n",
    "        logging.error(f\"Failed to parse report page: {url}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        report_dict = {}\n",
    "\n",
    "        # Extract 'Report Number'\n",
    "        report_header = report.find('span', class_='reportheader')\n",
    "        report_dict['Report Number'] = report_header.text.strip() if report_header else 'N/A'\n",
    "\n",
    "        # Extract 'Report Classification'\n",
    "        report_class = report.find('span', class_='reportclassification')\n",
    "        report_dict['Report Class'] = report_class.text.strip() if report_class else 'N/A'\n",
    "\n",
    "        # Extract additional fields\n",
    "        fields = report.find_all('span', class_='field')\n",
    "        for field in fields:\n",
    "            # Get the full text of the parent element\n",
    "            text = field.parent.text.strip()\n",
    "\n",
    "            # Only process fields in the format \"Header: Value\"\n",
    "            if ':' in text:\n",
    "                # Split into field name and value\n",
    "                field_name, value = text.split(':', 1)\n",
    "\n",
    "                # Clean up field name and value\n",
    "                field_name = field_name.strip().lower()\n",
    "                value = value.strip()\n",
    "\n",
    "                # Validate input (ensure no line breaks in the value)\n",
    "                if len(value.split('\\n')) == 1:\n",
    "                    # Store the field and value in the dictionary\n",
    "                    report_dict[field_name] = value\n",
    "\n",
    "        return report_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing report {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Reports\n",
    "The path to each Bigfoot sighting listing follows a structured hierarchy: GDB/State/County/Listing.\n",
    "- The GDB (Geographic Database of Bigfoot Sightings and Reports) serves as the starting point. It contains several tables, each linking to a state (or province, etc.) page.\n",
    "- Each state page features similar tables that provide links to county pages.\n",
    "- County pages only exist if there are recorded Bigfoot sightings in that county. These pages link to the county's sighting listings.\n",
    "- Finally, county sightings pages list all recorded sightings for that county, with links and short descriptions for each report.\n",
    "\n",
    "Having created the function to extract data from individual listings, we will now work in reverse of the above heiarchy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_county(url):\n",
    "    \"\"\"\n",
    "    Scrapes all report URLs from a county sightings page.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the county sightings page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing data from individual Bigfoot sighting reports.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting to scrape county page: {url}\")\n",
    "    report_dictionaries = []\n",
    "    base_url = 'https://www.bfro.net/GDB/'\n",
    "\n",
    "    # Fetch the county page\n",
    "    county_soup = soupify_website(url)\n",
    "    if not county_soup:\n",
    "        logging.error(f\"Failed to fetch or parse county page: {url}\")\n",
    "        return report_dictionaries\n",
    "\n",
    "    # Find all report links on the county page\n",
    "    county_reports = county_soup.find_all('span', class_='reportcaption')\n",
    "    report_urls = get_links(county_reports)\n",
    "    logging.info(f\"Found {len(report_urls)} reports on county page: {url}\")\n",
    "\n",
    "    # Process each report URL\n",
    "    for report_url in report_urls:\n",
    "        full_url = base_url + report_url\n",
    "        try:\n",
    "            logging.debug(f\"Scraping report: {full_url}\")\n",
    "            report_dict = create_sighting_dictionary(full_url)\n",
    "            if report_dict:\n",
    "                report_dictionaries.append(report_dict)\n",
    "            else:\n",
    "                logging.warning(f\"No data extracted for report: {full_url}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping report {full_url}: {e}\")\n",
    "\n",
    "    logging.info(f\"Finished scraping county page: {url}\")\n",
    "    return report_dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with Douglas County NE, should have 3 listings as of 12/3/2024\n",
    "county_url = 'https://www.bfro.net/GDB/show_county_reports.asp?state=NE&county=Douglas'\n",
    "sighting_url = 'https://www.bfro.net/GDB/'\n",
    "my_reports = scrape_county(county_url)\n",
    "print(f\"Douglass County has has {len(my_reports)} sightings\")\n",
    "print(\"Printing sighting dictionaries:\")\n",
    "for report in my_reports:\n",
    "    print(report)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_state(state_url):\n",
    "    state_soup = soupify_website(state_url)\n",
    "    tables = state_soup.find_all('table', class_='countytbl')\n",
    "\n",
    "    county_links = get_links(tables)\n",
    "\n",
    "    state_reports = []\n",
    "\n",
    "    for link in county_links:\n",
    "        sighting_links = scrape_county(sighting_url+link)\n",
    "        for sighting in sighting_links:\n",
    "            state_reports.append(sighting)\n",
    "    return state_reports           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_soup = soupify_website('https://www.bfro.net/GDB')\n",
    "gdb_tables = gdb_soup.find_all('table', class_='countytbl')\n",
    "state_urls = [x for x in get_links(gdb_tables) if 'state=int' not in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Big Show\n",
    "It's time to combine our functions to pull every report from the GDB. This is several \n",
    "thousand pages and will take a while so I included logging so that you can see how far the\n",
    "code has proceeded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reports = []\n",
    "\n",
    "for url in state_urls:\n",
    "    full_url = 'https://www.bfro.net' + url\n",
    "    logging.info(f\"Scraping state page: {full_url}\")\n",
    "    try:\n",
    "        # Scrape the state page for reports\n",
    "        state_dicts = scrape_state(full_url)\n",
    "\n",
    "        # Append each report dictionary to the master list\n",
    "        all_reports.extend(state_dicts)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping state page {full_url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We found a total of {len(all_reports)} entries.\")\n",
    "\n",
    "# I dont want to run this for an hour again so we're saving \n",
    "with open(\"data/raw_scraping_data.json\", \"w\") as file:\n",
    "    json.dump(all_reports, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
